import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Conditional imports for heavy dependencies
try:
    import spacy
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False
    logger.warning("spacy not available - NLP features will be disabled")

try:
    import flair
    FLAIR_AVAILABLE = True
except ImportError:
    FLAIR_AVAILABLE = False
    logger.warning("flair not available - some NLP features will be disabled")

class OllamaModel:
    def __init__(self, model_name: str, port: int = 11434):
        self.base_url = f"http://host.docker.internal:{port}"
        self.model_name = model_name

try:
    logger.info("üöÄ Chargement des mod√®les NLP...")

    # Initialisation des variables par d√©faut
    nlp = None
    bert_large = None
    
    # Chargement des mod√®les spaCy seulement si disponible
    if SPACY_AVAILABLE:
        try:
            nlp = spacy.load("fr_core_news_sm")
        except OSError:
            logger.warning("Mod√®le fr_core_news_sm non trouv√©, tentative avec fr_core_news_md")
            try:
                nlp = spacy.load("fr_core_news_md")
            except OSError:
                logger.warning("Mod√®le fr_core_news_md non trouv√©, utilisation du mod√®le anglais par d√©faut")
                try:
                    nlp = spacy.load("en_core_web_sm")
                except OSError:
                    logger.warning("Aucun mod√®le spaCy disponible")
                    nlp = None
    
    if FLAIR_AVAILABLE:
        bert_large = "dbmdz/bert-large-cased-finetuned-conll03-english"
    
    # Configuration du mod√®le d'embedding (port 11434 par d√©faut d'Ollama)
    minilm_model = OllamaModel("nomic-embed-text:latest", port=11434)

    logger.info("‚úÖ Mod√®les charg√©s avec succ√®s !")

except Exception as e:
    logger.error(f"‚ùå Erreur lors du chargement des mod√®les : {e}")
    # Ne pas lever d'exception pour permettre √† l'app de d√©marrer
    logger.warning("Application d√©marr√©e en mode d√©grad√© sans mod√®les NLP")
    nlp = None
    bert_large = None
    minilm_model = OllamaModel("nomic-embed-text:latest", port=11434)
